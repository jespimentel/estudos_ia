{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "axOeulNODJAZ"
      },
      "source": [
        "# Atividades da Semana 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vukcl7nXC-Ok"
      },
      "source": [
        "Crie um Notebook no Google Colab com o nome [ELT575_Semana2_<Matrícula>], modificando o campo <Matrícula> pela sua matrícula do Sapiens. As questões devem ser inseridas e respondidas no próprio notebook, sendo seguindas pelas implementações requeridas pela atividade. Ao final, envie o link do notebook como resposta da tarefa (lembre-se de deixar o acesso ao notebook público). Tente organizar o notebook em seções."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qpW0dfKDkuU"
      },
      "source": [
        "## Parte 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_o9d59JDn74"
      },
      "source": [
        "1 - Explique o que são e o funcionamento das Redes Neurais Convolucionais (CNN), enfatizando as principais diferenças entre esse tipo de modelo e as redes neurais MLPs?\n",
        "\n",
        "As redes neurais convolucionais diferem das redes neurais multicamadas porque, enquanto estas possuem camadas totalmente conectadas, as primeiras possuem, em sua arquitetura, as chamadas camadas convolucionais, cujos neurônios se conectam apenas em parte das saídas da camanda precedente.\n",
        "\n",
        "Essa concepção faz com que as camadas convolucionais se ocupem das características de baixo nível das camadas antecedentes e as repasse para a camada de nível superior. \n",
        "\n",
        "Note-se que, assim sendo, demandam-se menos ajustes de parâmetros, e, consequentemente, menor tempo de treinamento e poder computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84NsvVQcDs7f"
      },
      "source": [
        "2 - Como ocorrem o compartilhamento de parâmetros e a esparsidade em uma CNN?\n",
        "\n",
        "O compartilhameno de parâmetros ocorre na operação de convolução. O filtro (kernel), que nada mais é do que uma matriz de pesos, desliza por toda a imagem sem que seus parâmetros sejam alterados. Assim, um filtro com n canais aprendidos para determinada camada convolucional não se altera para todos os campos receptivos locais das entradas daquela camada, garantindo melhor eficiência do modelo. Desse modo, as características da imagem vão sendo extraídas e mapeadas, produzindo-se, na saída, o que se convencionou chamar de feature map.\n",
        "\n",
        "A esparsividade diz respeito ao fato de que, diversamente do que ocorre com as MLPs, nem todos os neurônios de uma camada (convolucional) estão ligados aos neurônios da camada antecedente. Com essa técnica, menos parâmetros precisam ser ajustados. Diz-se, assim, que a rede se torna mais esparsa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4mc4Y73Dw_K"
      },
      "source": [
        "3 - Qual a função da camada Max Pooling? Por que tal função desta camada é importante?\n",
        "\n",
        "As camadas de Max Pooling têm a finalidade de criar uma subamostragem dos valores da camada precedente. Camadas assim reduzem a resolução espacial da imagem de entrada, requisitando menos recursos computacionais. O processo compreende a divisão das imagens em partes e a extração dos valores mais significativos de cada parte. Isso faz com que as principais características da imagem seja preservadas na opeação. Por outro lado, garante que o modelo não seja tão influenciado por pequenas variações da imagem, como uma pequena alteração de posição do objeto. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghPDb2K6D0-8"
      },
      "source": [
        "4 - As técnicas de regularização são bastante utilizadas para tentar evitar o sobreajuste (overfitting) do modelo. Uma regularização bastante adotada em CNNs é o Dropout, sendo um método bastante simples e eficaz. Explique no que consiste a regularização Dropout e como implementá-la.\n",
        "\n",
        "O Dropout é uma técnica que desativa parte dos neurônios nas etapas de treinamento, fazendo com que seus pesos não se alterem nessas etapas. Isso faz com que os demais neurônios executem a tarefa do neurônio \"desligado\", criando uma redundância.\n",
        "\n",
        "Para implementar uma camada de dropout no TensorFlow, basta usar a classe tf.keras.layers.Dropout(). Dentro dos parênteses () poderíamos indicar o índice 0.2, por exemplo, que indicaria o \"desligamento\" de 20% dos neurônios em cada etapa do treinamento. \n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MVL2aY_DfWK"
      },
      "source": [
        "## Parte 2\n",
        "\n",
        "Link para notebook Image_classification_CNN: https://colab.research.google.com/drive/1r0pIj6INqqhnOwctukEYmih84GTTTiqN?usp=sharing\n",
        "\n",
        "Repita o exemplo Image_classification_CNN usando o dataset CIFAR-10 (https://www.tensorflow.org/datasets/catalog/cifar10), também disponível no Tensorflow. As imagens agora tem a forma 32 x 32 x 3, sendo necessário fazer ajustes na rede para esse caso. Recomendado executar usando o recurso de GPU do Colab agilizar os treinamentos das redes. Houve muita diferença no desempenho da rede treinada quando comparado ao exemplo?\n",
        "\n",
        "Experimente adicionar mais dois blocos de camadas Conv2D (64 filtros 3x3 - relu) + MaxPooling (2 x 2) antes da camada Flatten e compare os resultados.\n",
        "\n",
        "Seguindo o modelo modificado no item anterior, experimente adicionar um Dropout de 20% após cada camada MaxPooling. Comente os resultados obtidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mepYIv5SXA6q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFGaN5sRF4mI"
      },
      "source": [
        "### Cifar 10\n",
        "O cifar10 é um conjunto de dados com 60.000 imagens coloridas (50.000 de treinamento e 10.000 de teste), de formato 32 x 32 x 3. Estão distribuídas em 10 classes de 6.000 imagens cada.\n",
        "\n",
        "Fonte: https://www.tensorflow.org/datasets/catalog/cifar10?hl=pt-br"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oTJGJS7_oQq",
        "outputId": "a1149361-f51c-4a50-b44d-c542f2a56bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Carregando o dataset\n",
        "(trainX, trainY), (testX, testY) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIhMNN9u8063"
      },
      "outputs": [],
      "source": [
        "# fazendo um reshape no dataset para ter um único canal\n",
        "trainX = trainX.reshape((trainX.shape[0], 32, 32, 3))\n",
        "testX = testX.reshape((testX.shape[0], 32, 32, 3))\n",
        "\n",
        "# Convertendo valores dos pixels em float\n",
        "trainX = trainX.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "\n",
        "# normalização para escala [0-1]\n",
        "trainX = trainX / 255.0\n",
        "testX = testX / 255.0\n",
        "\n",
        "# transformando a variável alvo (target) para uma codificação one hot\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suXiZFlbAmmi"
      },
      "outputs": [],
      "source": [
        "# definindo modelo\n",
        "def define_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  \n",
        "  # compilando modelo\n",
        "  opt = Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vNMJ1WpeUNgp",
        "outputId": "6983226f-67d6-4b25-ef70-a181f2204188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 55s 43ms/step - loss: 1.5566 - accuracy: 0.4373 - val_loss: 1.3319 - val_accuracy: 0.5311\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 1.2563 - accuracy: 0.5576 - val_loss: 1.2345 - val_accuracy: 0.5630\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1507 - accuracy: 0.5948 - val_loss: 1.1734 - val_accuracy: 0.5897\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0700 - accuracy: 0.6241 - val_loss: 1.1801 - val_accuracy: 0.5875\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 1.0112 - accuracy: 0.6465 - val_loss: 1.1836 - val_accuracy: 0.5867\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 48s 39ms/step - loss: 0.9607 - accuracy: 0.6630 - val_loss: 1.1308 - val_accuracy: 0.6109\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 49s 39ms/step - loss: 0.9159 - accuracy: 0.6787 - val_loss: 1.1243 - val_accuracy: 0.6155\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 49s 40ms/step - loss: 0.8718 - accuracy: 0.6946 - val_loss: 1.1457 - val_accuracy: 0.6133\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 0.8377 - accuracy: 0.7067 - val_loss: 1.1073 - val_accuracy: 0.6266\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.7966 - accuracy: 0.7233 - val_loss: 1.0791 - val_accuracy: 0.6407\n",
            "> 64.070\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 51s 40ms/step - loss: 1.4830 - accuracy: 0.4700 - val_loss: 1.2885 - val_accuracy: 0.5426\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1929 - accuracy: 0.5803 - val_loss: 1.1444 - val_accuracy: 0.5965\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 1.0479 - accuracy: 0.6345 - val_loss: 1.1385 - val_accuracy: 0.6023\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 53s 42ms/step - loss: 0.9495 - accuracy: 0.6675 - val_loss: 1.0661 - val_accuracy: 0.6318\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 42s 34ms/step - loss: 0.8690 - accuracy: 0.6937 - val_loss: 1.0376 - val_accuracy: 0.6404\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 0.7892 - accuracy: 0.7235 - val_loss: 1.1019 - val_accuracy: 0.6313\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 0.7174 - accuracy: 0.7481 - val_loss: 1.0987 - val_accuracy: 0.6369\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 43s 35ms/step - loss: 0.6531 - accuracy: 0.7718 - val_loss: 1.0888 - val_accuracy: 0.6399\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 47s 37ms/step - loss: 0.5868 - accuracy: 0.7953 - val_loss: 1.1013 - val_accuracy: 0.6475\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.5258 - accuracy: 0.8153 - val_loss: 1.2103 - val_accuracy: 0.6350\n",
            "> 63.500\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.4817 - accuracy: 0.4727 - val_loss: 1.2587 - val_accuracy: 0.5568\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 1.1810 - accuracy: 0.5847 - val_loss: 1.1959 - val_accuracy: 0.5748\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 1.0454 - accuracy: 0.6340 - val_loss: 1.1532 - val_accuracy: 0.5960\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.9585 - accuracy: 0.6627 - val_loss: 1.0819 - val_accuracy: 0.6228\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.8729 - accuracy: 0.6927 - val_loss: 1.0737 - val_accuracy: 0.6299\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 0.8055 - accuracy: 0.7155 - val_loss: 1.1364 - val_accuracy: 0.6203\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.7384 - accuracy: 0.7403 - val_loss: 1.0755 - val_accuracy: 0.6405\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 0.6750 - accuracy: 0.7623 - val_loss: 1.0967 - val_accuracy: 0.6349\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 0.6149 - accuracy: 0.7860 - val_loss: 1.1368 - val_accuracy: 0.6359\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 43s 35ms/step - loss: 0.5621 - accuracy: 0.8027 - val_loss: 1.1879 - val_accuracy: 0.6294\n",
            "> 62.940\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4761 - accuracy: 0.4747 - val_loss: 1.3152 - val_accuracy: 0.5353\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 44s 36ms/step - loss: 1.1817 - accuracy: 0.5857 - val_loss: 1.1750 - val_accuracy: 0.5739\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0507 - accuracy: 0.6304 - val_loss: 1.1154 - val_accuracy: 0.6049\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9552 - accuracy: 0.6637 - val_loss: 1.0654 - val_accuracy: 0.6300\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 44s 35ms/step - loss: 0.8787 - accuracy: 0.6913 - val_loss: 1.0829 - val_accuracy: 0.6225\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.8146 - accuracy: 0.7125 - val_loss: 1.0782 - val_accuracy: 0.6300\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.7467 - accuracy: 0.7378 - val_loss: 1.1430 - val_accuracy: 0.6225\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.6892 - accuracy: 0.7577 - val_loss: 1.0966 - val_accuracy: 0.6386\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 43s 35ms/step - loss: 0.6297 - accuracy: 0.7790 - val_loss: 1.1740 - val_accuracy: 0.6252\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 43s 34ms/step - loss: 0.5759 - accuracy: 0.7983 - val_loss: 1.1865 - val_accuracy: 0.6294\n",
            "> 62.940\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4976 - accuracy: 0.4654 - val_loss: 1.2773 - val_accuracy: 0.5572\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.2160 - accuracy: 0.5755 - val_loss: 1.2099 - val_accuracy: 0.5788\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 1.0989 - accuracy: 0.6167 - val_loss: 1.1527 - val_accuracy: 0.5970\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 43s 35ms/step - loss: 1.0197 - accuracy: 0.6442 - val_loss: 1.1458 - val_accuracy: 0.5989\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.9484 - accuracy: 0.6703 - val_loss: 1.1222 - val_accuracy: 0.6160\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.8945 - accuracy: 0.6890 - val_loss: 1.1451 - val_accuracy: 0.6094\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 48s 38ms/step - loss: 0.8403 - accuracy: 0.7044 - val_loss: 1.1125 - val_accuracy: 0.6220\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.7931 - accuracy: 0.7221 - val_loss: 1.2095 - val_accuracy: 0.5909\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.7447 - accuracy: 0.7392 - val_loss: 1.1435 - val_accuracy: 0.6148\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.7007 - accuracy: 0.7542 - val_loss: 1.1266 - val_accuracy: 0.6311\n",
            "> 63.110\n",
            "Acurácia: média=63.312 desvio=0.431\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "scores = [] \n",
        "histories = []\n",
        "\n",
        "# definindo a validação k-fold\n",
        "kfold = KFold(5, shuffle=True, random_state=1)\n",
        "\n",
        "# loop para as k-folds (cada fold usa subconjuntos de treino e teste diferentes)\n",
        "for train_ix, test_ix in kfold.split(trainX):\n",
        "  \n",
        "  model = define_model()\n",
        "  \n",
        "  # recorta dados de acordo com índices da k-fold\n",
        "  train_data, train_target, val_data, val_target = trainX[train_ix], trainY[train_ix], trainX[test_ix], trainY[test_ix]\n",
        "  \n",
        "  # treinamento do modelo\n",
        "  history = model.fit(train_data, train_target, \n",
        "                      epochs=10, batch_size=32, \n",
        "                      validation_data=(val_data, val_target), \n",
        "                      verbose=1)\n",
        "  \n",
        "  # desempenho do modelo\n",
        "  _, acc = model.evaluate(val_data, val_target, verbose=0)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "  \n",
        "  # armazena resultados de cada modelo treinado dentro da k-fold\n",
        "  scores.append(acc)\n",
        "  histories.append(history)\n",
        "\n",
        "print('Acurácia: média=%.3f desvio=%.3f' % (np.mean(scores)*100, np.std(scores)*100))\n",
        "plt.boxplot(scores)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "-kQjP09ZfjEF",
        "outputId": "f412d6b9-4410-4ae8-fad6-5db3980b90b3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(testX[15,:,:,0], cmap='gray');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_MpGPWwfnGE",
        "outputId": "6dcadd99-8f66-4a1e-c23e-7bbc83d52f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 137ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(model.predict(np.expand_dims(testX[15], axis=0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkNkvtE0IRk1"
      },
      "source": [
        "Experimente adicionar mais dois blocos de camadas Conv2D (64 filtros 3x3 - relu) + MaxPooling (2 x 2) antes da camada Flatten e compare os resultados.\n",
        "\n",
        "\n",
        "Seguindo o modelo modificado no item anterior, experimente adicionar um Dropout de 20% após cada camada MaxPooling. Comente os resultados obtidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaqJxSdfIGQn"
      },
      "outputs": [],
      "source": [
        "# Adicionando Conv2D (64 filtros 3x3 - relu) + MaxPooling (2 x 2)\n",
        "def define_model_1():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  \n",
        "  # compilando modelo\n",
        "  opt = Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eqCx46PgJN1N",
        "outputId": "191b2c84-ae98-4555-8bf7-367532725ccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 68s 54ms/step - loss: 1.4837 - accuracy: 0.4642 - val_loss: 1.2101 - val_accuracy: 0.5711\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 1.1289 - accuracy: 0.6028 - val_loss: 1.0629 - val_accuracy: 0.6202\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 67s 53ms/step - loss: 0.9832 - accuracy: 0.6583 - val_loss: 0.9877 - val_accuracy: 0.6573\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 66s 52ms/step - loss: 0.8842 - accuracy: 0.6918 - val_loss: 0.9541 - val_accuracy: 0.6698\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.8054 - accuracy: 0.7175 - val_loss: 0.9666 - val_accuracy: 0.6639\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.7327 - accuracy: 0.7449 - val_loss: 0.8980 - val_accuracy: 0.6887\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 66s 53ms/step - loss: 0.6657 - accuracy: 0.7679 - val_loss: 0.9218 - val_accuracy: 0.6839\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.6088 - accuracy: 0.7874 - val_loss: 0.9683 - val_accuracy: 0.6715\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.5564 - accuracy: 0.8042 - val_loss: 0.9487 - val_accuracy: 0.6914\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 63s 51ms/step - loss: 0.5083 - accuracy: 0.8201 - val_loss: 0.9554 - val_accuracy: 0.6944\n",
            "> 69.440\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 66s 52ms/step - loss: 1.4798 - accuracy: 0.4695 - val_loss: 1.1602 - val_accuracy: 0.5947\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 67s 53ms/step - loss: 1.1234 - accuracy: 0.6051 - val_loss: 1.0416 - val_accuracy: 0.6340\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 66s 53ms/step - loss: 0.9779 - accuracy: 0.6594 - val_loss: 1.0514 - val_accuracy: 0.6320\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 68s 54ms/step - loss: 0.8906 - accuracy: 0.6884 - val_loss: 0.9629 - val_accuracy: 0.6626\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 67s 53ms/step - loss: 0.8086 - accuracy: 0.7193 - val_loss: 1.0011 - val_accuracy: 0.6576\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 68s 54ms/step - loss: 0.7408 - accuracy: 0.7414 - val_loss: 0.9389 - val_accuracy: 0.6841\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 72s 57ms/step - loss: 0.6765 - accuracy: 0.7630 - val_loss: 0.9631 - val_accuracy: 0.6825\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 64s 52ms/step - loss: 0.6165 - accuracy: 0.7826 - val_loss: 1.0388 - val_accuracy: 0.6744\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 64s 51ms/step - loss: 0.5639 - accuracy: 0.8031 - val_loss: 1.0660 - val_accuracy: 0.6618\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 63s 50ms/step - loss: 0.5109 - accuracy: 0.8215 - val_loss: 1.0374 - val_accuracy: 0.6773\n",
            "> 67.730\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 68s 54ms/step - loss: 1.5396 - accuracy: 0.4426 - val_loss: 1.2590 - val_accuracy: 0.5462\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 1.1749 - accuracy: 0.5848 - val_loss: 1.1798 - val_accuracy: 0.5857\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 66s 53ms/step - loss: 1.0147 - accuracy: 0.6437 - val_loss: 1.0399 - val_accuracy: 0.6328\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 69s 55ms/step - loss: 0.9235 - accuracy: 0.6786 - val_loss: 0.9563 - val_accuracy: 0.6734\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 74s 59ms/step - loss: 0.8498 - accuracy: 0.7044 - val_loss: 0.9620 - val_accuracy: 0.6679\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 69s 55ms/step - loss: 0.7951 - accuracy: 0.7234 - val_loss: 0.9389 - val_accuracy: 0.6828\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 69s 55ms/step - loss: 0.7409 - accuracy: 0.7403 - val_loss: 0.9406 - val_accuracy: 0.6917\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 68s 54ms/step - loss: 0.6869 - accuracy: 0.7600 - val_loss: 0.9123 - val_accuracy: 0.6947\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 68s 54ms/step - loss: 0.6395 - accuracy: 0.7776 - val_loss: 0.9393 - val_accuracy: 0.6948\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.5930 - accuracy: 0.7946 - val_loss: 0.9561 - val_accuracy: 0.6982\n",
            "> 69.820\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 69s 54ms/step - loss: 1.5027 - accuracy: 0.4575 - val_loss: 1.2541 - val_accuracy: 0.5564\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 1.1501 - accuracy: 0.5941 - val_loss: 1.1522 - val_accuracy: 0.5960\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 66s 53ms/step - loss: 1.0088 - accuracy: 0.6474 - val_loss: 1.0368 - val_accuracy: 0.6428\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.9093 - accuracy: 0.6831 - val_loss: 1.1029 - val_accuracy: 0.6182\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.8347 - accuracy: 0.7085 - val_loss: 0.9487 - val_accuracy: 0.6764\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.7689 - accuracy: 0.7308 - val_loss: 0.9250 - val_accuracy: 0.6823\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 62s 50ms/step - loss: 0.7063 - accuracy: 0.7521 - val_loss: 0.9804 - val_accuracy: 0.6729\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 64s 51ms/step - loss: 0.6452 - accuracy: 0.7742 - val_loss: 0.9185 - val_accuracy: 0.6908\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 66s 52ms/step - loss: 0.5938 - accuracy: 0.7907 - val_loss: 0.9988 - val_accuracy: 0.6794\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 64s 51ms/step - loss: 0.5416 - accuracy: 0.8096 - val_loss: 0.9969 - val_accuracy: 0.6893\n",
            "> 68.930\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 67s 52ms/step - loss: 1.4844 - accuracy: 0.4670 - val_loss: 1.2264 - val_accuracy: 0.5682\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 66s 53ms/step - loss: 1.1211 - accuracy: 0.6043 - val_loss: 1.1527 - val_accuracy: 0.6000\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 62s 50ms/step - loss: 0.9871 - accuracy: 0.6571 - val_loss: 1.0464 - val_accuracy: 0.6322\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 64s 51ms/step - loss: 0.8942 - accuracy: 0.6899 - val_loss: 1.0239 - val_accuracy: 0.6396\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 63s 51ms/step - loss: 0.8172 - accuracy: 0.7131 - val_loss: 0.9519 - val_accuracy: 0.6712\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 63s 51ms/step - loss: 0.7381 - accuracy: 0.7461 - val_loss: 0.9625 - val_accuracy: 0.6755\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 63s 51ms/step - loss: 0.6774 - accuracy: 0.7667 - val_loss: 0.9933 - val_accuracy: 0.6650\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 63s 50ms/step - loss: 0.6161 - accuracy: 0.7848 - val_loss: 0.9892 - val_accuracy: 0.6777\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 65s 52ms/step - loss: 0.5597 - accuracy: 0.8046 - val_loss: 0.9983 - val_accuracy: 0.6826\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 66s 53ms/step - loss: 0.5034 - accuracy: 0.8260 - val_loss: 1.0479 - val_accuracy: 0.6752\n",
            "> 67.520\n",
            "Acurácia: média=68.688 desvio=0.915\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "scores = [] \n",
        "histories = []\n",
        "\n",
        "# definindo a validação k-fold\n",
        "kfold = KFold(5, shuffle=True, random_state=1)\n",
        "\n",
        "# loop para as k-folds (cada fold usa subconjuntos de treino e teste diferentes)\n",
        "for train_ix, test_ix in kfold.split(trainX):\n",
        "  \n",
        "  model = define_model_1()\n",
        "  \n",
        "  # recorta dados de acordo com índices da k-fold\n",
        "  train_data, train_target, val_data, val_target = trainX[train_ix], trainY[train_ix], trainX[test_ix], trainY[test_ix]\n",
        "  \n",
        "  # treinamento do modelo\n",
        "  history = model.fit(train_data, train_target, \n",
        "                      epochs=10, batch_size=32, \n",
        "                      validation_data=(val_data, val_target), \n",
        "                      verbose=1)\n",
        "  \n",
        "  # desempenho do modelo\n",
        "  _, acc = model.evaluate(val_data, val_target, verbose=0)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "  \n",
        "  # armazena resultados de cada modelo treinado dentro da k-fold\n",
        "  scores.append(acc)\n",
        "  histories.append(history)\n",
        "\n",
        "print('Acurácia: média=%.3f desvio=%.3f' % (np.mean(scores)*100, np.std(scores)*100))\n",
        "plt.boxplot(scores)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15PXOA_DIvr9"
      },
      "outputs": [],
      "source": [
        "# Adicionando um Dropout de 20% após cada camada MaxPooling\n",
        "def define_model_2():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  \n",
        "  # compilando modelo\n",
        "  opt = Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "palk_YSTKRxH",
        "outputId": "82e4337e-3522-44ba-b662-103a6f7e2760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 73s 58ms/step - loss: 1.5780 - accuracy: 0.4252 - val_loss: 1.2749 - val_accuracy: 0.5583\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 71s 57ms/step - loss: 1.2183 - accuracy: 0.5698 - val_loss: 1.0922 - val_accuracy: 0.6207\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 76s 61ms/step - loss: 1.0853 - accuracy: 0.6178 - val_loss: 1.0348 - val_accuracy: 0.6377\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 72s 58ms/step - loss: 0.9976 - accuracy: 0.6534 - val_loss: 1.0315 - val_accuracy: 0.6353\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 70s 56ms/step - loss: 0.9343 - accuracy: 0.6751 - val_loss: 0.9525 - val_accuracy: 0.6641\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 82s 65ms/step - loss: 0.8772 - accuracy: 0.6943 - val_loss: 1.0418 - val_accuracy: 0.6347\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 85s 68ms/step - loss: 0.8362 - accuracy: 0.7042 - val_loss: 0.9022 - val_accuracy: 0.6887\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 80s 64ms/step - loss: 0.7929 - accuracy: 0.7217 - val_loss: 0.8792 - val_accuracy: 0.6984\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 87s 69ms/step - loss: 0.7574 - accuracy: 0.7340 - val_loss: 0.8622 - val_accuracy: 0.7025\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 77s 62ms/step - loss: 0.7215 - accuracy: 0.7466 - val_loss: 0.8669 - val_accuracy: 0.7019\n",
            "> 70.190\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 71s 56ms/step - loss: 1.5312 - accuracy: 0.4460 - val_loss: 1.3526 - val_accuracy: 0.5155\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 73s 58ms/step - loss: 1.2057 - accuracy: 0.5732 - val_loss: 1.1702 - val_accuracy: 0.5992\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 72s 57ms/step - loss: 1.0828 - accuracy: 0.6187 - val_loss: 1.0161 - val_accuracy: 0.6485\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 78s 63ms/step - loss: 0.9969 - accuracy: 0.6492 - val_loss: 0.9662 - val_accuracy: 0.6672\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 75s 60ms/step - loss: 0.9331 - accuracy: 0.6761 - val_loss: 0.9421 - val_accuracy: 0.6728\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 73s 58ms/step - loss: 0.8714 - accuracy: 0.6950 - val_loss: 0.9082 - val_accuracy: 0.6868\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 75s 60ms/step - loss: 0.8249 - accuracy: 0.7086 - val_loss: 0.8816 - val_accuracy: 0.6931\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 81s 65ms/step - loss: 0.7796 - accuracy: 0.7268 - val_loss: 0.9308 - val_accuracy: 0.6789\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 77s 62ms/step - loss: 0.7472 - accuracy: 0.7371 - val_loss: 0.8455 - val_accuracy: 0.7100\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 75s 60ms/step - loss: 0.7120 - accuracy: 0.7489 - val_loss: 0.8471 - val_accuracy: 0.7101\n",
            "> 71.010\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 78s 62ms/step - loss: 1.5775 - accuracy: 0.4293 - val_loss: 1.3535 - val_accuracy: 0.5165\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 78s 62ms/step - loss: 1.2371 - accuracy: 0.5619 - val_loss: 1.0964 - val_accuracy: 0.6185\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 76s 61ms/step - loss: 1.0897 - accuracy: 0.6165 - val_loss: 1.0522 - val_accuracy: 0.6357\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 72s 58ms/step - loss: 1.0084 - accuracy: 0.6462 - val_loss: 1.0075 - val_accuracy: 0.6536\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 74s 59ms/step - loss: 0.9476 - accuracy: 0.6692 - val_loss: 0.9377 - val_accuracy: 0.6765\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 70s 56ms/step - loss: 0.8983 - accuracy: 0.6838 - val_loss: 0.9130 - val_accuracy: 0.6873\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 77s 62ms/step - loss: 0.8576 - accuracy: 0.6998 - val_loss: 0.8950 - val_accuracy: 0.6923\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 71s 57ms/step - loss: 0.8141 - accuracy: 0.7153 - val_loss: 0.8837 - val_accuracy: 0.6973\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 74s 59ms/step - loss: 0.7845 - accuracy: 0.7229 - val_loss: 0.8669 - val_accuracy: 0.7047\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 70s 56ms/step - loss: 0.7579 - accuracy: 0.7325 - val_loss: 0.8821 - val_accuracy: 0.6997\n",
            "> 69.970\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 82s 63ms/step - loss: 1.5605 - accuracy: 0.4347 - val_loss: 1.2831 - val_accuracy: 0.5523\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 75s 60ms/step - loss: 1.2127 - accuracy: 0.5723 - val_loss: 1.2067 - val_accuracy: 0.5809\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 74s 59ms/step - loss: 1.0910 - accuracy: 0.6190 - val_loss: 1.0134 - val_accuracy: 0.6564\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 73s 58ms/step - loss: 1.0002 - accuracy: 0.6524 - val_loss: 0.9730 - val_accuracy: 0.6656\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 75s 60ms/step - loss: 0.9442 - accuracy: 0.6697 - val_loss: 0.9672 - val_accuracy: 0.6741\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 72s 58ms/step - loss: 0.9008 - accuracy: 0.6855 - val_loss: 0.9638 - val_accuracy: 0.6682\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 74s 59ms/step - loss: 0.8608 - accuracy: 0.6996 - val_loss: 0.9000 - val_accuracy: 0.6954\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 74s 60ms/step - loss: 0.8213 - accuracy: 0.7112 - val_loss: 0.9006 - val_accuracy: 0.6913\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 81s 65ms/step - loss: 0.7862 - accuracy: 0.7248 - val_loss: 0.8836 - val_accuracy: 0.6986\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 71s 57ms/step - loss: 0.7605 - accuracy: 0.7325 - val_loss: 0.9088 - val_accuracy: 0.6891\n",
            "> 68.910\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 72s 57ms/step - loss: 1.5445 - accuracy: 0.4462 - val_loss: 1.3036 - val_accuracy: 0.5357\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 69s 55ms/step - loss: 1.1846 - accuracy: 0.5809 - val_loss: 1.0896 - val_accuracy: 0.6188\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 70s 56ms/step - loss: 1.0630 - accuracy: 0.6257 - val_loss: 1.0310 - val_accuracy: 0.6337\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 72s 58ms/step - loss: 0.9771 - accuracy: 0.6560 - val_loss: 0.9843 - val_accuracy: 0.6498\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 71s 57ms/step - loss: 0.9255 - accuracy: 0.6750 - val_loss: 0.9547 - val_accuracy: 0.6651\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 70s 56ms/step - loss: 0.8709 - accuracy: 0.6932 - val_loss: 0.9804 - val_accuracy: 0.6586\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 70s 56ms/step - loss: 0.8305 - accuracy: 0.7077 - val_loss: 0.9295 - val_accuracy: 0.6761\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 71s 57ms/step - loss: 0.7919 - accuracy: 0.7206 - val_loss: 0.9204 - val_accuracy: 0.6758\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 72s 58ms/step - loss: 0.7514 - accuracy: 0.7360 - val_loss: 0.8903 - val_accuracy: 0.6936\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 71s 57ms/step - loss: 0.7189 - accuracy: 0.7448 - val_loss: 0.8845 - val_accuracy: 0.6931\n",
            "> 69.310\n",
            "Acurácia: média=69.878 desvio=0.728\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "scores = [] \n",
        "histories = []\n",
        "\n",
        "# definindo a validação k-fold\n",
        "kfold = KFold(5, shuffle=True, random_state=1)\n",
        "\n",
        "# loop para as k-folds (cada fold usa subconjuntos de treino e teste diferentes)\n",
        "for train_ix, test_ix in kfold.split(trainX):\n",
        "  \n",
        "  model = define_model_2()\n",
        "  \n",
        "  # recorta dados de acordo com índices da k-fold\n",
        "  train_data, train_target, val_data, val_target = trainX[train_ix], trainY[train_ix], trainX[test_ix], trainY[test_ix]\n",
        "  \n",
        "  # treinamento do modelo\n",
        "  history = model.fit(train_data, train_target, \n",
        "                      epochs=10, batch_size=32, \n",
        "                      validation_data=(val_data, val_target), \n",
        "                      verbose=1)\n",
        "  \n",
        "  # desempenho do modelo\n",
        "  _, acc = model.evaluate(val_data, val_target, verbose=0)\n",
        "  print('> %.3f' % (acc * 100.0))\n",
        "  \n",
        "  # armazena resultados de cada modelo treinado dentro da k-fold\n",
        "  scores.append(acc)\n",
        "  histories.append(history)\n",
        "\n",
        "print('Acurácia: média=%.3f desvio=%.3f' % (np.mean(scores)*100, np.std(scores)*100))\n",
        "plt.boxplot(scores)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVOcyWQu7kse"
      },
      "source": [
        "## Conclusão\n",
        "\n",
        "Para a resolução do exercício, partimos do modelo-base, assim considerado o anteriormente utilizado com o dataset MNIST. \n",
        "\n",
        "Para a utilização do modelo com o dataset CIFAR-10 foi necessário adequar os parâmetros da primeira camada (Conv2D), para adequá-lo ao formato das figuras (32x32x3).\n",
        "\n",
        "Depois de compilado, rodamos o modelo-base.\n",
        "\n",
        "Para a avaliação do modelo, usamos a técnica da validação cruzada K-fold. O conjunto de dados foi embaralhado (shuffle=True) e dividido por 5 vezes (n_splits), calculando-se, em cada uma delas, a acurácia, com a função model.evaluate().\n",
        "\n",
        "Os valores foram adicionados a uma lista, da qual se extraíram a média e o desvio-padrão. O boxplot resultante também foi exibido.\n",
        "\n",
        "O modelo-base apresentou acurácia média de 63.31 %, com desvio-padrão de 0.43\n",
        "\n",
        "Adicionando Conv2D (64 filtros 3x3 - relu) + MaxPooling (2 x 2), observamos acurácia de 68.69 % e desvio-padrão de 0,92.\n",
        "\n",
        "Adicionando um Dropout de 20% após cada camada MaxPooling, obtivemos acurácia média de 69.88 % e desvio-padrão de 0,73."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
